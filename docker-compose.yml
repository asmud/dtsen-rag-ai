# ==============================================================================
# RAG CHATBOT - 3 DEPLOYMENT PROFILES
# ==============================================================================
#
# 1. APPLE SILICON - For Mac M2/M3/M4 Pro/Max/Ultra
#    Usage: docker-compose --profile apple-silicon up -d
#    Features: ARM64 optimized, unified memory management, Apple Silicon specific tuning
#    Services: ollama + chatbot (both optimized for Apple Silicon)
#
# 2. NVIDIA GPU - For systems with NVIDIA GPUs and Docker GPU support
#    Usage: docker-compose --profile nvidia-gpu up -d  
#    Features: CUDA acceleration, higher memory/CPU limits, GPU-optimized embeddings
#    Services: ollama-nvidia + chatbot-nvidia (both GPU accelerated)
#    Requirements: NVIDIA Docker runtime, compatible GPU
#
# 3. CPU-ONLY (FALLBACK) - For any system without GPU acceleration
#    Usage: docker-compose --profile cpu-only up -d
#    Features: Conservative resource usage, maximum compatibility, CPU-only processing
#    Services: ollama-cpu + chatbot-cpu (both CPU optimized)
#    Best for: VMs, older hardware, development environments
#
# Background processing with Celery (optional for any profile):
#   docker-compose --profile with-celery up -d
# ==============================================================================

services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: dtsen_rag_postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-rag_db}
      POSTGRES_USER: ${POSTGRES_USER:-rag_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-rag_pass}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U rag_user -d rag_db"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7-alpine
    container_name: dtsen_rag_redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apple Silicon optimized Ollama service (DEFAULT for Mac M2/M3/M4)
  ollama:
    image: asmud/ollama:latest-arm64
    container_name: dtsen_rag_ollama_apple
    platform: linux/arm64
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4  # Optimized for Apple Silicon multi-core
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_KEEP_ALIVE=5m  # Efficient memory management for unified memory
      - OLLAMA_MAX_QUEUE=10
      - OLLAMA_FLASH_ATTENTION=0  # Disabled for ARM compatibility
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: ["CMD", "nc", "-zv", "localhost", "11434"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Allow time for model downloads
    profiles:
      - apple-silicon

  # Apple Silicon M2 Pro optimizations
  chatbot:
    build: ./app
    container_name: dtsen_rag_chatbot
    platform: linux/arm64  # Optimized for Apple Silicon
    deploy:
      resources:
        limits:
          memory: 4G  # Reasonable limit for unified memory
          cpus: '6.0'  # Utilize M2 Pro efficiently
    env_file:
      - .env
    environment:
      # Override specific settings for Apple Silicon deployment
      - PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.8
      - OMP_NUM_THREADS=8
    
    ports:
      - "${API_PORT:-8000}:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    volumes:
      - ./app:/app
      - ./data:/app/data
      - ./backups:/app/backups
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/', timeout=5).read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s #allow embed model to load
    restart: unless-stopped
    profiles:
      - apple-silicon

  # NVIDIA GPU optimized Ollama service (for systems with NVIDIA GPUs)
  ollama-nvidia:
    image: asmud/ollama:latest
    container_name: dtsen_rag_ollama_nvidia
    platform: linux/amd64
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=8  # Higher for GPU acceleration
      - OLLAMA_MAX_LOADED_MODELS=3  # Can handle more models with GPU
      - OLLAMA_KEEP_ALIVE=10m  # Longer keep-alive for GPU efficiency
      - OLLAMA_MAX_QUEUE=20
      - OLLAMA_FLASH_ATTENTION=1  # Enable for NVIDIA GPUs
    volumes:
      - ollama_data_nvidia:/root/.ollama
    healthcheck:
      test: ["CMD", "nc", "-zv", "localhost", "11434"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - nvidia-gpu

  # NVIDIA GPU optimized chatbot service
  chatbot-nvidia:
    build: ./app
    container_name: dtsen_rag_chatbot_nvidia
    platform: linux/amd64
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          memory: 8G  # Higher memory for GPU workloads
          cpus: '8.0'  # More CPU cores for GPU systems
    env_file:
      - .env
    environment:
      # Override specific settings for NVIDIA GPU deployment
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=all
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    
    ports:
      - "${API_PORT:-8000}:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama-nvidia:
        condition: service_healthy
    volumes:
      - ./app:/app
      - ./data:/app/data
      - ./backups:/app/backups
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/', timeout=5).read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s #allow embed model to load
    restart: unless-stopped
    profiles:
      - nvidia-gpu

  # CPU-only Ollama service (Enhanced for x86_64 performance)
  ollama-cpu:
    image: asmud/ollama:latest
    platform: linux/amd64
    container_name: dtsen_rag_ollama_cpu
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    restart: unless-stopped
    # Enhanced CPU-only configuration optimized for x86_64 systems
    deploy:
      resources:
        limits:
          memory: 8G  # Maximum memory allocation for better performance
          cpus: '6.0'  # Increased CPU cores for x86_64 optimization
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4  # Increased parallelism for multi-core x86_64
      - OLLAMA_MAX_LOADED_MODELS=1  # Focus on single model for performance
      - OLLAMA_KEEP_ALIVE=10m  # Longer keep-alive to reduce model reloading
      - OLLAMA_MAX_QUEUE=15  # Higher queue for better throughput
      - OLLAMA_FLASH_ATTENTION=0  # Disabled for CPU compatibility
      - OLLAMA_MAX_VRAM=0  # Force CPU-only processing
      - OMP_NUM_THREADS=6  # Optimize OpenMP for 6 CPU cores
    volumes:
      - ollama_data_cpu:/root/.ollama
    healthcheck:
      test: ["CMD", "nc", "-zv", "localhost", "11434"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 150s
    profiles:
      - cpu-only

  # CPU-only chatbot service (Enhanced for x86_64 performance)
  chatbot-cpu:
    build: ./app
    platform: linux/amd64
    container_name: dtsen_rag_chatbot_cpu
    deploy:
      resources:
        limits:
          memory: 8G  # Maximum memory allocation for better performance
          cpus: '6.0'  # Increased CPU cores for x86_64 optimization
    env_file:
      - .env
    environment:
      # Enhanced settings for x86_64 CPU-only deployment
      - CPU_WORKERS=12  # Maximized workers for better CPU utilization
      - OMP_NUM_THREADS=6  # Optimize OpenMP threading
      - MKL_NUM_THREADS=6  # Optimize Intel MKL if available
      - NUMBA_NUM_THREADS=6  # Optimize Numba threading
      - OPENBLAS_NUM_THREADS=6  # Optimize OpenBLAS threading
    
    ports:
      - "${API_PORT:-8000}:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama-cpu:
        condition: service_healthy
    volumes:
      - ./data:/app/data
      - ./backups:/app/backups
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/', timeout=5).read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s #allow models to load in CPU-only mode
    restart: unless-stopped
    profiles:
      - cpu-only

  # Optional: Celery worker for background tasks
  celery-worker:
    build: ./app
    container_name: dtsen_rag_celery_worker
    command: celery -A tasks.celery_app worker --loglevel=info
    env_file:
      - .env
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./data:/app/data
    restart: unless-stopped
    profiles:
      - with-celery

volumes:
  pg_data:
  redis_data:
  ollama_data:        # Apple Silicon default
  ollama_data_nvidia: # NVIDIA GPU specific
  ollama_data_cpu:    # CPU-only fallback
