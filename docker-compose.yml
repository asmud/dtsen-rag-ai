# ==============================================================================
# RAG CHATBOT - 3 DEPLOYMENT PROFILES
# ==============================================================================
#
# 1. APPLE SILICON - For Mac M2/M3/M4 Pro/Max/Ultra
#    Usage: docker-compose --profile apple-silicon up -d
#    Features: ARM64 optimized, unified memory management, Apple Silicon specific tuning
#    Services: ollama + chatbot (both optimized for Apple Silicon)
#
# 2. NVIDIA GPU - For systems with NVIDIA GPUs and Docker GPU support
#    Usage: docker-compose --profile nvidia-gpu up -d  
#    Features: CUDA acceleration, higher memory/CPU limits, GPU-optimized embeddings
#    Services: ollama-nvidia + chatbot-nvidia (both GPU accelerated)
#    Requirements: NVIDIA Docker runtime, compatible GPU
#
# 3. CPU-ONLY (FALLBACK) - For any system without GPU acceleration
#    Usage: docker-compose --profile cpu-only up -d
#    Features: Conservative resource usage, maximum compatibility, CPU-only processing
#    Services: ollama-cpu + chatbot-cpu (both CPU optimized)
#    Best for: VMs, older hardware, development environments
#
# Background processing with Celery (optional for any profile):
#   docker-compose --profile with-celery up -d
# ==============================================================================

services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: dtsen_rag_postgres
    environment:
      POSTGRES_DB: rag_db
      POSTGRES_USER: rag_user
      POSTGRES_PASSWORD: rag_pass
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U rag_user -d rag_db"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7-alpine
    container_name: dtsen_rag_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apple Silicon optimized Ollama service (DEFAULT for Mac M2/M3/M4)
  ollama:
    image: asmud/ollama:latest-arm64
    container_name: dtsen_rag_ollama_apple
    platform: linux/arm64
    ports:
      - "11434:11434"
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4  # Optimized for Apple Silicon multi-core
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_KEEP_ALIVE=5m  # Efficient memory management for unified memory
      - OLLAMA_MAX_QUEUE=10
      - OLLAMA_FLASH_ATTENTION=0  # Disabled for ARM compatibility
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: ["CMD", "nc", "-zv", "localhost", "11434"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Allow time for model downloads
    profiles:
      - apple-silicon

  # Apple Silicon M2 Pro optimizations
  chatbot:
    build: ./app
    container_name: dtsen_rag_chatbot
    platform: linux/arm64  # Optimized for Apple Silicon
    deploy:
      resources:
        limits:
          memory: 4G  # Reasonable limit for unified memory
          cpus: '6.0'  # Utilize M2 Pro efficiently
    env_file:
      - .env.apple-silicon
    environment:
      # Override specific settings for Apple Silicon deployment
      - PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.8
      - OMP_NUM_THREADS=8
    
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    volumes:
      - ./app:/app
      - ./data:/app/data
      - ./backups:/app/backups
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/', timeout=5).read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s #allow embed model to load
    restart: unless-stopped
    profiles:
      - apple-silicon

  # NVIDIA GPU optimized Ollama service (for systems with NVIDIA GPUs)
  ollama-nvidia:
    image: asmud/ollama:latest
    container_name: dtsen_rag_ollama_nvidia
    platform: linux/amd64
    ports:
      - "11434:11434"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=8  # Higher for GPU acceleration
      - OLLAMA_MAX_LOADED_MODELS=3  # Can handle more models with GPU
      - OLLAMA_KEEP_ALIVE=10m  # Longer keep-alive for GPU efficiency
      - OLLAMA_MAX_QUEUE=20
      - OLLAMA_FLASH_ATTENTION=1  # Enable for NVIDIA GPUs
    volumes:
      - ollama_data_nvidia:/root/.ollama
    healthcheck:
      test: ["CMD", "nc", "-zv", "localhost", "11434"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - nvidia-gpu

  # NVIDIA GPU optimized chatbot service
  chatbot-nvidia:
    build: ./app
    container_name: dtsen_rag_chatbot_nvidia
    platform: linux/amd64
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          memory: 8G  # Higher memory for GPU workloads
          cpus: '8.0'  # More CPU cores for GPU systems
    env_file:
      - .env.nvidia-gpu
    environment:
      # Override specific settings for NVIDIA GPU deployment
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=all
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama-nvidia:
        condition: service_healthy
    volumes:
      - ./app:/app
      - ./data:/app/data
      - ./backups:/app/backups
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/', timeout=5).read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s #allow embed model to load
    restart: unless-stopped
    profiles:
      - nvidia-gpu

  # CPU-only Ollama service (fallback for any system without GPU)
  ollama-cpu:
    image: asmud/ollama:latest
    platform: linux/amd64
    container_name: dtsen_rag_ollama_cpu
    ports:
      - "11434:11434"
    restart: unless-stopped
    # Conservative CPU-only configuration for maximum compatibility
    deploy:
      resources:
        limits:
          memory: 2G  # Conservative memory limit
          cpus: '2.0'  # Conservative CPU allocation
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=1  # Conservative for CPU-only
      - OLLAMA_MAX_LOADED_MODELS=1  # Single model for memory efficiency
      - OLLAMA_KEEP_ALIVE=2m  # Shorter keep-alive to free memory
      - OLLAMA_MAX_QUEUE=5
      - OLLAMA_FLASH_ATTENTION=0  # Disabled for CPU compatibility
    volumes:
      - ollama_data_cpu:/root/.ollama
    healthcheck:
      test: ["CMD", "nc", "-zv", "localhost", "11434"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 150s
    profiles:
      - cpu-only

  # CPU-only chatbot service
  chatbot-cpu:
    build: ./app
    platform: linux/amd64
    container_name: dtsen_rag_chatbot_cpu
    env_file:
      - .env.cpu-only
    environment:
      # Override specific settings for CPU-only deployment
      - CPU_WORKERS=4
    
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama-cpu:
        condition: service_healthy
    volumes:
      - ./data:/app/data
      - ./backups:/app/backups
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/', timeout=5).read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s #allow models to load in CPU-only mode
    restart: unless-stopped
    profiles:
      - cpu-only

  # Optional: Celery worker for background tasks
  celery-worker:
    build: ./app
    container_name: dtsen_rag_celery_worker
    command: celery -A tasks.celery_app worker --loglevel=info
    environment:
      # Same environment as chatbot but only what's needed for worker
      - DATABASE_URL=postgresql://rag_user:rag_pass@postgres:5432/rag_db
      - OLLAMA_API=http://ollama:11434
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
      - ENABLE_CACHING=true
      - LOG_LEVEL=INFO
      - DATA_DIR=/app/data
      - EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - EMBEDDING_DEVICE=cpu
      - MAX_CHUNK_SIZE=512
      - CHUNK_OVERLAP=50
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./data:/app/data
    restart: unless-stopped
    profiles:
      - with-celery

volumes:
  pg_data:
  redis_data:
  ollama_data:        # Apple Silicon default
  ollama_data_nvidia: # NVIDIA GPU specific
  ollama_data_cpu:    # CPU-only fallback
